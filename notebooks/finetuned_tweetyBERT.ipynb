{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/george-vengrovski/anaconda3/envs/canary-vae/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sys.path.append(\"src\")\n",
    "os.chdir('/home/george-vengrovski/Documents/projects/tweety_bert_paper')\n",
    "\n",
    "from data_class import SongDataSet_Image, CollateFunction\n",
    "from model import TweetyBERT\n",
    "from analysis import plot_umap_projection\n",
    "from utils import detailed_count_parameters, load_weights, load_model\n",
    "\n",
    "\n",
    "weights_path = \"/home/george-vengrovski/Documents/projects/tweety_bert_paper/temp_storage/10mask_500_cluster_relative/saved_weights/model_step_49995.pth\"\n",
    "config_path = \"/home/george-vengrovski/Documents/projects/tweety_bert_paper/temp_storage/10mask_500_cluster_relative/config.json\"\n",
    "\n",
    "tweety_bert_model = load_model(config_path, weights_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from data_class import SongDataSet_Image\n",
    "\n",
    "train_dir = \"/home/george-vengrovski/Documents/projects/tweety_bert_paper/files/test\"\n",
    "test_dir = \"/home/george-vengrovski/Documents/projects/tweety_bert_paper/files/train\"\n",
    "\n",
    "train_dataset = SongDataSet_Image(train_dir, num_classes=21, psuedo_labels_generated=False)\n",
    "test_dataset = SongDataSet_Image(test_dir, num_classes=21, psuedo_labels_generated=False)\n",
    "\n",
    "collate_fn = CollateFunction(segment_length=1000)  # Adjust the segment length if needed\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Linear Classifier and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_probe import LinearProbeModel, LinearProbeTrainer\n",
    "\n",
    "classifier_model = LinearProbeModel(num_classes=21, model_type=\"neural_net\", model=tweety_bert_model, freeze_layers=True, layer_num=-1, layer_id=\"attention_output\", classifier_dims=384)\n",
    "classifier_model = classifier_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100: FER = 50.39%, Train Loss = 1.9892, Val Loss = 2.1295\n",
      "Batch 200: FER = 51.49%, Train Loss = 1.8678, Val Loss = 2.0234\n",
      "Batch 300: FER = 49.62%, Train Loss = 2.1090, Val Loss = 2.0318\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[39m=\u001b[39m LinearProbeTrainer(model\u001b[39m=\u001b[39mclassifier_model, train_loader\u001b[39m=\u001b[39mtrain_loader, test_loader\u001b[39m=\u001b[39mtest_loader, device\u001b[39m=\u001b[39mdevice, lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m, plotting\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, batches_per_eval\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, desired_total_batches\u001b[39m=\u001b[39m\u001b[39m1e4\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/Documents/projects/tweety_bert_paper/src/linear_probe.py:162\u001b[0m, in \u001b[0;36mLinearProbeTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m total_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    161\u001b[0m \u001b[39mif\u001b[39;00m total_batches \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatches_per_eval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 162\u001b[0m     avg_val_loss, avg_frame_error \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidate_model()\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_tqdm: \n\u001b[1;32m    164\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mBatch \u001b[39m\u001b[39m{\u001b[39;00mtotal_batches\u001b[39m}\u001b[39;00m\u001b[39m: FER = \u001b[39m\u001b[39m{\u001b[39;00mavg_frame_error\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%, Train Loss = \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Val Loss = \u001b[39m\u001b[39m{\u001b[39;00mavg_val_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/projects/tweety_bert_paper/src/linear_probe.py:122\u001b[0m, in \u001b[0;36mLinearProbeTrainer.validate_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m num_val_batches \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    121\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 122\u001b[0m     \u001b[39mfor\u001b[39;00m i, (spectrogram, label, _) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_loader):\n\u001b[1;32m    123\u001b[0m         \u001b[39mif\u001b[39;00m i \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatches_per_eval:\n\u001b[1;32m    124\u001b[0m             \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/canary-vae/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/canary-vae/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/canary-vae/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/canary-vae/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Documents/projects/tweety_bert_paper/src/data_class.py:27\u001b[0m, in \u001b[0;36mSongDataSet_Image.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     24\u001b[0m file_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_path[index]\n\u001b[1;32m     26\u001b[0m data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(file_path, allow_pickle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 27\u001b[0m spectogram \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     29\u001b[0m ground_truth_labels \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     31\u001b[0m \u001b[39m# for the cases when this dataclass is used on datasets that have not had psuedo labels generated for them\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39m# such an example may be in the eval process \u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/canary-vae/lib/python3.11/site-packages/numpy/lib/npyio.py:253\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mif\u001b[39;00m magic \u001b[39m==\u001b[39m \u001b[39mformat\u001b[39m\u001b[39m.\u001b[39mMAGIC_PREFIX:\n\u001b[1;32m    252\u001b[0m     \u001b[39mbytes\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzip\u001b[39m.\u001b[39mopen(key)\n\u001b[0;32m--> 253\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39m\u001b[39m.\u001b[39mread_array(\u001b[39mbytes\u001b[39m,\n\u001b[1;32m    254\u001b[0m                              allow_pickle\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mallow_pickle,\n\u001b[1;32m    255\u001b[0m                              pickle_kwargs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpickle_kwargs,\n\u001b[1;32m    256\u001b[0m                              max_header_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_header_size)\n\u001b[1;32m    257\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzip\u001b[39m.\u001b[39mread(key)\n",
      "File \u001b[0;32m~/anaconda3/envs/canary-vae/lib/python3.11/site-packages/numpy/lib/format.py:823\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    821\u001b[0m             read_count \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(max_read_count, count \u001b[39m-\u001b[39m i)\n\u001b[1;32m    822\u001b[0m             read_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(read_count \u001b[39m*\u001b[39m dtype\u001b[39m.\u001b[39mitemsize)\n\u001b[0;32m--> 823\u001b[0m             data \u001b[39m=\u001b[39m _read_bytes(fp, read_size, \u001b[39m\"\u001b[39m\u001b[39marray data\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    824\u001b[0m             array[i:i\u001b[39m+\u001b[39mread_count] \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mfrombuffer(data, dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m    825\u001b[0m                                                      count\u001b[39m=\u001b[39mread_count)\n\u001b[1;32m    827\u001b[0m \u001b[39mif\u001b[39;00m fortran_order:\n",
      "File \u001b[0;32m~/anaconda3/envs/canary-vae/lib/python3.11/site-packages/numpy/lib/format.py:958\u001b[0m, in \u001b[0;36m_read_bytes\u001b[0;34m(fp, size, error_template)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    954\u001b[0m     \u001b[39m# io files (default in python3) return None or raise on\u001b[39;00m\n\u001b[1;32m    955\u001b[0m     \u001b[39m# would-block, python2 file will truncate, probably nothing can be\u001b[39;00m\n\u001b[1;32m    956\u001b[0m     \u001b[39m# done about that.  note that regular files can't be non-blocking\u001b[39;00m\n\u001b[1;32m    957\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 958\u001b[0m         r \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39mread(size \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(data))\n\u001b[1;32m    959\u001b[0m         data \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m r\n\u001b[1;32m    960\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(r) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m==\u001b[39m size:\n",
      "File \u001b[0;32m~/anaconda3/envs/canary-vae/lib/python3.11/zipfile.py:955\u001b[0m, in \u001b[0;36mZipExtFile.read\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_offset \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    954\u001b[0m \u001b[39mwhile\u001b[39;00m n \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof:\n\u001b[0;32m--> 955\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read1(n)\n\u001b[1;32m    956\u001b[0m     \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(data):\n\u001b[1;32m    957\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_readbuffer \u001b[39m=\u001b[39m data\n",
      "File \u001b[0;32m~/anaconda3/envs/canary-vae/lib/python3.11/zipfile.py:1045\u001b[0m, in \u001b[0;36mZipExtFile._read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_left \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1044\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m-> 1045\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_crc(data)\n\u001b[1;32m   1046\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/canary-vae/lib/python3.11/zipfile.py:970\u001b[0m, in \u001b[0;36mZipExtFile._update_crc\u001b[0;34m(self, newdata)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expected_crc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    968\u001b[0m     \u001b[39m# No need to compute the CRC if we don't have a reference value\u001b[39;00m\n\u001b[1;32m    969\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 970\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_running_crc \u001b[39m=\u001b[39m crc32(newdata, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_running_crc)\n\u001b[1;32m    971\u001b[0m \u001b[39m# Check the CRC if we're at the end of the file\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_running_crc \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expected_crc:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = LinearProbeTrainer(model=classifier_model, train_loader=train_loader, test_loader=test_loader, device=device, lr=1e-3, plotting=True, batches_per_eval=100, desired_total_batches=1e4, patience=4)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 133/133 [00:17<00:00,  7.63batch/s]\n"
     ]
    }
   ],
   "source": [
    "from linear_probe import ModelEvaluator\n",
    "\n",
    "evaluator = ModelEvaluator(classifier_model, test_loader)\n",
    "class_frame_error_rates, total_frame_error_rate = evaluator.validate_model_multiple_passes(num_passes=1, max_batches=1250)\n",
    "evaluator.save_results(class_frame_error_rates, total_frame_error_rate, '/home/george-vengrovski/Documents/projects/tweety_bert_paper/results/test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canary-vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
