{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Configuration file not found at /home/george-vengrovski/Documents/experiments_backup/TweetyBERT-Combined-MSE-1/config.json",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/projects/tweety_bert_paper/src/utils.py:50\u001b[0m, in \u001b[0;36mload_config\u001b[0;34m(config_path)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(config_path, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     51\u001b[0m         config \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/george-vengrovski/Documents/experiments_backup/TweetyBERT-Combined-MSE-1/config.json'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m weights_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/home/george-vengrovski/Documents/experiments_backup/TweetyBERT-Combined-MSE-1/saved_weights/model_step_11800.pth\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m config_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/home/george-vengrovski/Documents/experiments_backup/TweetyBERT-Combined-MSE-1/config.json\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 17\u001b[0m tweety_bert_model \u001b[39m=\u001b[39m load_model(config_path, weights_path)\n\u001b[1;32m     19\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/projects/tweety_bert_paper/src/utils.py:67\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(config_path, weight_path)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_model\u001b[39m(config_path, weight_path\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     57\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39m    Initialize and load the model with the given configuration and weights.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m    torch.nn.Module: The initialized model.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m     config \u001b[39m=\u001b[39m load_config(config_path)\n\u001b[1;32m     69\u001b[0m     model \u001b[39m=\u001b[39m TweetyBERT(\n\u001b[1;32m     70\u001b[0m         d_transformer\u001b[39m=\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39md_transformer\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[1;32m     71\u001b[0m         nhead_transformer\u001b[39m=\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mnhead_transformer\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m         length\u001b[39m=\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     82\u001b[0m     )\n\u001b[1;32m     84\u001b[0m     \u001b[39mif\u001b[39;00m weight_path:\n",
      "File \u001b[0;32m~/Documents/projects/tweety_bert_paper/src/utils.py:54\u001b[0m, in \u001b[0;36mload_config\u001b[0;34m(config_path)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[39mreturn\u001b[39;00m config\n\u001b[1;32m     53\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConfiguration file not found at \u001b[39m\u001b[39m{\u001b[39;00mconfig_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Configuration file not found at /home/george-vengrovski/Documents/experiments_backup/TweetyBERT-Combined-MSE-1/config.json"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "sys.path.append(\"src\")\n",
    "os.chdir('/home/george-vengrovski/Documents/projects/tweety_bert_paper')\n",
    "\n",
    "from utils import load_model\n",
    "\n",
    "weights_path = \"/home/george-vengrovski/Documents/experiments_backup/tweety_bert_paper/experiments/TweetyBERT-Combined-MSE-1/saved_weights/model_step_11800.pth\"\n",
    "config_path = \"/home/george-vengrovski/Documents/experiments_backup/tweety_bert_paper/experiments/TweetyBERT-Combined-MSE-1/config.json\"\n",
    "\n",
    "tweety_bert_model = load_model(config_path, weights_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from data_class import SongDetectorDataClass, CollateFunctionSongDetection\n",
    "\n",
    "train_dir = \"/home/george-vengrovski/Documents/data/finetune_labeled_data_train\"\n",
    "test_dir = \"/home/george-vengrovski/Documents/data/finetune_labeled_data_test\"\n",
    "\n",
    "train_dataset = SongDetectorDataClass(train_dir, num_classes=2, psuedo_labels_generated=False)\n",
    "test_dataset = SongDetectorDataClass(test_dir, num_classes=2, psuedo_labels_generated=False)\n",
    "\n",
    "collate_fn = CollateFunctionSongDetection(segment_length=1000)  # Adjust the segment length if needed\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Linear Classifier and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_probe import LinearProbeModel, LinearProbeTrainer\n",
    "\n",
    "classifier_model = LinearProbeModel(num_classes=2, model_type=\"neural_net\", model=tweety_bert_model, freeze_layers=True, layer_num=-1, layer_id=\"attention_output\", classifier_dims=384)\n",
    "classifier_model = classifier_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = LinearProbeTrainer(model=classifier_model, train_loader=train_loader, test_loader=test_loader, device=device, lr=1e-5, plotting=True, batches_per_eval=1, desired_total_batches=1e3, patience=15)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_probe import ModelEvaluator\n",
    "\n",
    "evaluator = ModelEvaluator(classifier_model, test_loader)\n",
    "class_frame_error_rates, total_frame_error_rate = evaluator.validate_model_multiple_passes(num_passes=1, max_batches=1250)\n",
    "evaluator.save_results(class_frame_error_rates, total_frame_error_rate, '/home/george-vengrovski/Documents/projects/tweety_bert_paper/results/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Song and Not Song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def plot_spectrogram_with_labels_and_logits(spec, ground_truth_label, logits):\n",
    "    # Apply sigmoid to logits to scale them between 0 and 1\n",
    "    logits_sigmoid = sigmoid(logits.numpy())\n",
    "\n",
    "    # Scale logits to match the frequency axis of the spectrogram\n",
    "    freq_range = spec.shape[0]  # Assuming the frequency range is the first dimension of spec\n",
    "    logits_scaled = logits_sigmoid * freq_range  # Scale the logits to the spectrogram's frequency range\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    # Plot spectrogram\n",
    "    plt.imshow(spec.numpy(), aspect='auto', origin='lower')\n",
    "\n",
    "    # Overlay ground truth labels as a bar\n",
    "    song_bar = ground_truth_label.numpy()[:, 0]  # Assuming first column is for 'song'\n",
    "    not_song_bar = ground_truth_label.numpy()[:, 1]  # Assuming second column is for 'not song'\n",
    "    plt.fill_between(range(spec.shape[1]), -5, 0, where=song_bar > 0.5, color='green', step='mid', alpha=0.5, label='Not Song')\n",
    "    plt.fill_between(range(spec.shape[1]), -5, 0, where=not_song_bar > 0.5, color='red', step='mid', alpha=0.5, label='Song')\n",
    "\n",
    "    # Overlay logits as line plots\n",
    "    # Note: We add a small offset to avoid plotting directly on the bottom axis\n",
    "    plt.plot(logits_scaled[:, 0], color='cyan', label='Logits - Not Song')\n",
    "    plt.plot(logits_scaled[:, 1], color='magenta', label='Logits - Song')\n",
    "\n",
    "    plt.colorbar(label='Spectrogram Intensity')\n",
    "    plt.xlabel('Time Bins')\n",
    "    plt.ylabel('Frequency Bins')\n",
    "    plt.title('Spectrogram with Ground Truth and Logits')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "spec, ground_truth_label, _ = next(iter(test_loader))\n",
    "\n",
    "\n",
    "logits = classifier_model.forward(spec.to(device))\n",
    "\n",
    "# first batch \n",
    "spec = spec[0]\n",
    "ground_truth_label = ground_truth_label[0]\n",
    "logits = logits[0]\n",
    "\n",
    "# remove channel dims\n",
    "spec = spec[0]\n",
    "# Example usage with your data (convert tensors to CPU if on a different device)\n",
    "plot_spectrogram_with_labels_and_logits(spec.detach().cpu(), ground_truth_label.detach().cpu(), logits.detach().cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "src = \"/home/george-vengrovski/Documents/projects/tweety_bert_paper/files/sort_these\"\n",
    "\n",
    "files_iterator = iter(os.listdir(src))  # Create an iterator over the files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Ensure to run this part only after initializing `files_iterator` as shown above\n",
    "\n",
    "try:\n",
    "    file = next(files_iterator)  # Get the next file from the iterator\n",
    "    file_path = os.path.join(src, file)  # Full path to the file\n",
    "\n",
    "    # Load the spectrogram from the file\n",
    "    f = np.load(file_path, allow_pickle=True)\n",
    "    spec = f['s']\n",
    "    # spec = spec[20:216]  # Trimming the spectrogram\n",
    "\n",
    "    # # Z-score normalization\n",
    "    # spec_mean = spec.mean()\n",
    "    # spec_std = spec.std()\n",
    "    # spec_normalized = (spec - spec_mean) / spec_std\n",
    "\n",
    "    # Forward pass through the model (assuming `classifier_model` and `device` are already defined)\n",
    "    logits = classifier_model.forward(torch.Tensor(spec[:,:1000]).unsqueeze(0).unsqueeze(0).to(device))\n",
    "    logits = logits[0]\n",
    "\n",
    "    # Assuming `plot_spectrogram_with_labels_and_logits` and `ground_truth_label` are defined\n",
    "    plot_spectrogram_with_labels_and_logits(torch.Tensor(spec[:,:1000]), ground_truth_label.detach().cpu(), logits.detach().cpu())\n",
    "\n",
    "except StopIteration:\n",
    "    print(\"No more files to process.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm  # Import tqdm for progress tracking\n",
    "\n",
    "\n",
    "# Assuming `classifier_model` and `device` are already defined\n",
    "\n",
    "def process_spectrogram(spec, max_length=1000):\n",
    "    \"\"\"\n",
    "    Process the spectrogram in chunks, pass through the classifier, and return the combined logits.\n",
    "    \"\"\"\n",
    "    # Calculate the number of chunks needed\n",
    "    num_chunks = int(np.ceil(spec.shape[1] / max_length))\n",
    "    combined_logits = []\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        # Extract the chunk\n",
    "        start_idx = i * max_length\n",
    "        end_idx = min((i + 1) * max_length, spec.shape[1])\n",
    "        chunk = spec[:, start_idx:end_idx]\n",
    "\n",
    "        # # Normalize the chunk\n",
    "        # chunk_mean = chunk.mean()\n",
    "        # chunk_std = chunk.std()\n",
    "        # chunk_normalized = (chunk - chunk_mean) / chunk_std\n",
    "\n",
    "        # Forward pass through the model\n",
    "        logits = classifier_model.forward(torch.Tensor(chunk).unsqueeze(0).unsqueeze(0).to(device))\n",
    "        logits = logits[0]\n",
    "\n",
    "        # Convert logits to binary predictions\n",
    "        binary_logits = (logits[:, 1] > logits[:, 0]).long()\n",
    "\n",
    "        # Append the binary logits\n",
    "        combined_logits.append(binary_logits.detach().cpu().numpy())\n",
    "\n",
    "    # Concatenate all chunks' logits\n",
    "    final_logits = np.concatenate(combined_logits, axis=-1)\n",
    "\n",
    "    return final_logits\n",
    "\n",
    "def process_files(src):\n",
    "    \"\"\"\n",
    "    Process each file in the directory, reshape logits, and save them along with the original spectrogram.\n",
    "    \"\"\"\n",
    "    files = os.listdir(src)\n",
    "    for file in tqdm(files, desc=\"Processing files\"):  # Wrap the loop with tqdm for progress tracking\n",
    "        file_path = os.path.join(src, file)\n",
    "\n",
    "        try:\n",
    "            # Load the spectrogram from the file\n",
    "            f = np.load(file_path, allow_pickle=True)\n",
    "            spec = f['s']\n",
    "\n",
    "            # Process the spectrogram and get logits\n",
    "            logits = process_spectrogram(spec)\n",
    "\n",
    "            # Save the spectrogram and logits\n",
    "            save_path = os.path.join(src, f\"processed_{file}\")\n",
    "            np.savez(save_path, s=spec, song=logits)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process file {file}: {str(e)}\")\n",
    "\n",
    "\n",
    "src = \"/home/george-vengrovski/Documents/projects/tweety_bert_paper/files/sort_these\"\n",
    "process_files(src)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canary-vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
