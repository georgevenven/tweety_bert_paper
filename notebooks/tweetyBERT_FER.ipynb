{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "sys.path.append(\"src\")\n",
    "os.chdir('/home/george-vengrovski/Documents/projects/tweety_bert_paper')\n",
    "\n",
    "from data_class import CollateFunction\n",
    "from utils import load_model\n",
    "\n",
    "weights_path = \"/home/george-vengrovski/Documents/projects/tweety_bert_paper/experiments/TweetyBERT-cluster_LLB3_10_Mask_50_clusters/saved_weights/model_step_8600.pth\"\n",
    "config_path = \"/home/george-vengrovski/Documents/projects/tweety_bert_paper/experiments/TweetyBERT-cluster_LLB3_10_Mask_50_clusters/config.json\"\n",
    "\n",
    "tweety_bert_model = load_model(config_path, weights_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from data_class import SongDataSet_Image\n",
    "\n",
    "train_dir = \"/home/george-vengrovski/Documents/projects/tweety_bert_paper/files/llb3_train_50\"\n",
    "test_dir = \"/home/george-vengrovski/Documents/projects/tweety_bert_paper/files/llb3_test_50\"\n",
    "\n",
    "train_dataset = SongDataSet_Image(train_dir, num_classes=21, psuedo_labels_generated=False)\n",
    "test_dataset = SongDataSet_Image(test_dir, num_classes=21, psuedo_labels_generated=False)\n",
    "\n",
    "collate_fn = CollateFunction(segment_length=1000)  # Adjust the segment length if needed\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=48, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=48, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Linear Classifier and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_probe import LinearProbeModel, LinearProbeTrainer\n",
    "\n",
    "classifier_model = LinearProbeModel(num_classes=21, model_type=\"neural_net\", model=tweety_bert_model, freeze_layers=True, layer_num=-3, layer_id=\"attention_output\", classifier_dims=196)\n",
    "classifier_model = classifier_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/george-vengrovski/anaconda3/envs/canary-vae/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: FER = 57.14%, Train Loss = 3.1360, Val Loss = 2.3618\n",
      "Batch 2: FER = 34.88%, Train Loss = 2.2790, Val Loss = 1.7033\n",
      "Batch 3: FER = 30.90%, Train Loss = 1.6679, Val Loss = 1.3300\n",
      "Batch 4: FER = 24.63%, Train Loss = 1.3690, Val Loss = 1.0885\n",
      "Batch 5: FER = 23.03%, Train Loss = 1.2085, Val Loss = 0.9812\n",
      "Batch 6: FER = 21.49%, Train Loss = 0.9500, Val Loss = 0.8732\n",
      "Batch 7: FER = 20.13%, Train Loss = 0.9660, Val Loss = 0.7711\n",
      "Batch 8: FER = 19.38%, Train Loss = 0.8020, Val Loss = 0.7279\n",
      "Batch 9: FER = 16.43%, Train Loss = 0.7715, Val Loss = 0.6492\n",
      "Batch 10: FER = 17.02%, Train Loss = 0.6638, Val Loss = 0.6344\n",
      "Batch 11: FER = 16.51%, Train Loss = 0.7864, Val Loss = 0.6087\n",
      "Batch 12: FER = 16.79%, Train Loss = 0.6356, Val Loss = 0.6527\n",
      "Batch 13: FER = 15.67%, Train Loss = 0.5061, Val Loss = 0.6090\n",
      "Batch 14: FER = 14.13%, Train Loss = 0.6564, Val Loss = 0.5546\n",
      "Batch 15: FER = 14.41%, Train Loss = 0.5017, Val Loss = 0.5660\n",
      "Batch 16: FER = 13.88%, Train Loss = 0.4797, Val Loss = 0.5186\n",
      "Batch 17: FER = 12.12%, Train Loss = 0.5496, Val Loss = 0.4730\n",
      "Batch 18: FER = 11.74%, Train Loss = 0.5042, Val Loss = 0.4656\n",
      "Batch 19: FER = 12.35%, Train Loss = 0.5801, Val Loss = 0.5000\n",
      "Batch 20: FER = 12.42%, Train Loss = 0.5541, Val Loss = 0.5133\n",
      "Batch 21: FER = 11.35%, Train Loss = 0.5217, Val Loss = 0.4461\n",
      "Batch 22: FER = 12.21%, Train Loss = 0.4329, Val Loss = 0.4814\n",
      "Batch 23: FER = 9.02%, Train Loss = 0.3788, Val Loss = 0.3741\n",
      "Batch 24: FER = 12.20%, Train Loss = 0.5120, Val Loss = 0.4591\n",
      "Batch 25: FER = 11.15%, Train Loss = 0.4326, Val Loss = 0.4214\n",
      "Batch 26: FER = 10.22%, Train Loss = 0.4378, Val Loss = 0.3870\n",
      "Batch 27: FER = 10.62%, Train Loss = 0.4014, Val Loss = 0.4049\n",
      "Batch 28: FER = 10.10%, Train Loss = 0.3621, Val Loss = 0.3835\n",
      "Batch 29: FER = 9.85%, Train Loss = 0.3623, Val Loss = 0.3973\n",
      "Batch 30: FER = 9.45%, Train Loss = 0.3426, Val Loss = 0.3713\n",
      "Batch 31: FER = 10.12%, Train Loss = 0.3326, Val Loss = 0.3945\n",
      "Batch 32: FER = 9.13%, Train Loss = 0.3771, Val Loss = 0.3663\n",
      "Batch 33: FER = 8.86%, Train Loss = 0.2877, Val Loss = 0.3555\n",
      "Batch 34: FER = 7.26%, Train Loss = 0.4128, Val Loss = 0.2969\n",
      "Batch 35: FER = 7.95%, Train Loss = 0.3186, Val Loss = 0.3129\n",
      "Batch 36: FER = 10.25%, Train Loss = 0.3532, Val Loss = 0.3843\n",
      "Batch 37: FER = 9.38%, Train Loss = 0.3041, Val Loss = 0.3605\n",
      "Batch 38: FER = 7.82%, Train Loss = 0.2819, Val Loss = 0.3103\n",
      "Batch 39: FER = 8.67%, Train Loss = 0.2921, Val Loss = 0.3327\n",
      "Batch 40: FER = 8.89%, Train Loss = 0.3306, Val Loss = 0.3372\n",
      "Batch 41: FER = 8.30%, Train Loss = 0.3753, Val Loss = 0.3354\n",
      "Batch 42: FER = 8.13%, Train Loss = 0.2836, Val Loss = 0.3058\n",
      "Batch 43: FER = 9.51%, Train Loss = 0.2959, Val Loss = 0.3789\n",
      "Batch 44: FER = 7.79%, Train Loss = 0.3110, Val Loss = 0.2865\n",
      "Batch 45: FER = 8.99%, Train Loss = 0.2325, Val Loss = 0.3450\n",
      "Batch 46: FER = 7.52%, Train Loss = 0.3041, Val Loss = 0.2965\n",
      "Batch 47: FER = 8.80%, Train Loss = 0.3738, Val Loss = 0.3495\n",
      "Batch 48: FER = 8.25%, Train Loss = 0.2987, Val Loss = 0.3180\n",
      "Batch 49: FER = 9.16%, Train Loss = 0.3316, Val Loss = 0.3301\n",
      "Batch 50: FER = 8.91%, Train Loss = 0.3118, Val Loss = 0.3106\n",
      "Batch 51: FER = 7.83%, Train Loss = 0.3092, Val Loss = 0.2898\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[39m=\u001b[39m LinearProbeTrainer(model\u001b[39m=\u001b[39mclassifier_model, train_loader\u001b[39m=\u001b[39mtrain_loader, test_loader\u001b[39m=\u001b[39mtest_loader, device\u001b[39m=\u001b[39mdevice, lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m, plotting\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, batches_per_eval\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, desired_total_batches\u001b[39m=\u001b[39m\u001b[39m1e4\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/Documents/projects/tweety_bert_paper/src/linear_probe.py:169\u001b[0m, in \u001b[0;36mLinearProbeTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m total_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    168\u001b[0m \u001b[39mif\u001b[39;00m total_batches \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatches_per_eval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 169\u001b[0m     avg_val_loss, avg_frame_error \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidate_model()\n\u001b[1;32m    171\u001b[0m     raw_loss_list\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m    172\u001b[0m     raw_val_loss_list\u001b[39m.\u001b[39mappend(avg_val_loss)\n",
      "File \u001b[0;32m~/Documents/projects/tweety_bert_paper/src/linear_probe.py:123\u001b[0m, in \u001b[0;36mLinearProbeTrainer.validate_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m num_val_batches \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mfor\u001b[39;00m i, (spectrogram, label, _) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_loader):\n\u001b[1;32m    124\u001b[0m         \u001b[39mif\u001b[39;00m i \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatches_per_eval:\n\u001b[1;32m    125\u001b[0m             \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/canary-vae/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/canary-vae/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/canary-vae/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/canary-vae/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Documents/projects/tweety_bert_paper/src/data_class.py:22\u001b[0m, in \u001b[0;36mSongDataSet_Image.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     19\u001b[0m file_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_path[index]\n\u001b[1;32m     21\u001b[0m data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(file_path, allow_pickle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 22\u001b[0m spectogram \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     24\u001b[0m ground_truth_labels \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     26\u001b[0m \u001b[39m# for the cases when this dataclass is used on datasets that have not had psuedo labels generated for them\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m# such an example may be in the eval process \u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/canary-vae/lib/python3.11/site-packages/numpy/lib/npyio.py:253\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mif\u001b[39;00m magic \u001b[39m==\u001b[39m \u001b[39mformat\u001b[39m\u001b[39m.\u001b[39mMAGIC_PREFIX:\n\u001b[1;32m    252\u001b[0m     \u001b[39mbytes\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzip\u001b[39m.\u001b[39mopen(key)\n\u001b[0;32m--> 253\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39m\u001b[39m.\u001b[39mread_array(\u001b[39mbytes\u001b[39m,\n\u001b[1;32m    254\u001b[0m                              allow_pickle\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mallow_pickle,\n\u001b[1;32m    255\u001b[0m                              pickle_kwargs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpickle_kwargs,\n\u001b[1;32m    256\u001b[0m                              max_header_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_header_size)\n\u001b[1;32m    257\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzip\u001b[39m.\u001b[39mread(key)\n",
      "File \u001b[0;32m~/anaconda3/envs/canary-vae/lib/python3.11/site-packages/numpy/lib/format.py:823\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    821\u001b[0m             read_count \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(max_read_count, count \u001b[39m-\u001b[39m i)\n\u001b[1;32m    822\u001b[0m             read_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(read_count \u001b[39m*\u001b[39m dtype\u001b[39m.\u001b[39mitemsize)\n\u001b[0;32m--> 823\u001b[0m             data \u001b[39m=\u001b[39m _read_bytes(fp, read_size, \u001b[39m\"\u001b[39m\u001b[39marray data\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    824\u001b[0m             array[i:i\u001b[39m+\u001b[39mread_count] \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mfrombuffer(data, dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m    825\u001b[0m                                                      count\u001b[39m=\u001b[39mread_count)\n\u001b[1;32m    827\u001b[0m \u001b[39mif\u001b[39;00m fortran_order:\n",
      "File \u001b[0;32m~/anaconda3/envs/canary-vae/lib/python3.11/site-packages/numpy/lib/format.py:958\u001b[0m, in \u001b[0;36m_read_bytes\u001b[0;34m(fp, size, error_template)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    954\u001b[0m     \u001b[39m# io files (default in python3) return None or raise on\u001b[39;00m\n\u001b[1;32m    955\u001b[0m     \u001b[39m# would-block, python2 file will truncate, probably nothing can be\u001b[39;00m\n\u001b[1;32m    956\u001b[0m     \u001b[39m# done about that.  note that regular files can't be non-blocking\u001b[39;00m\n\u001b[1;32m    957\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 958\u001b[0m         r \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39mread(size \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(data))\n\u001b[1;32m    959\u001b[0m         data \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m r\n\u001b[1;32m    960\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(r) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m==\u001b[39m size:\n",
      "File \u001b[0;32m~/anaconda3/envs/canary-vae/lib/python3.11/zipfile.py:955\u001b[0m, in \u001b[0;36mZipExtFile.read\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_offset \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    954\u001b[0m \u001b[39mwhile\u001b[39;00m n \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof:\n\u001b[0;32m--> 955\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read1(n)\n\u001b[1;32m    956\u001b[0m     \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(data):\n\u001b[1;32m    957\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_readbuffer \u001b[39m=\u001b[39m data\n",
      "File \u001b[0;32m~/anaconda3/envs/canary-vae/lib/python3.11/zipfile.py:1045\u001b[0m, in \u001b[0;36mZipExtFile._read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_left \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1044\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m-> 1045\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_crc(data)\n\u001b[1;32m   1046\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/canary-vae/lib/python3.11/zipfile.py:970\u001b[0m, in \u001b[0;36mZipExtFile._update_crc\u001b[0;34m(self, newdata)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expected_crc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    968\u001b[0m     \u001b[39m# No need to compute the CRC if we don't have a reference value\u001b[39;00m\n\u001b[1;32m    969\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 970\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_running_crc \u001b[39m=\u001b[39m crc32(newdata, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_running_crc)\n\u001b[1;32m    971\u001b[0m \u001b[39m# Check the CRC if we're at the end of the file\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_running_crc \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expected_crc:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = LinearProbeTrainer(model=classifier_model, train_loader=train_loader, test_loader=test_loader, device=device, lr=1e-3, plotting=True, batches_per_eval=1, desired_total_batches=1e4, patience=4)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_probe import ModelEvaluator\n",
    "\n",
    "evaluator = ModelEvaluator(classifier_model, test_loader)\n",
    "class_frame_error_rates, total_frame_error_rate = evaluator.validate_model_multiple_passes(num_passes=1, max_batches=1250)\n",
    "evaluator.save_results(class_frame_error_rates, total_frame_error_rate, '/home/george-vengrovski/Documents/projects/tweety_bert_paper/results/test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canary-vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
