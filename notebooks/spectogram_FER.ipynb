{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/george-vengrovski/anaconda3/envs/tweetybert/lib/python3.11/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/george-vengrovski/anaconda3/envs/tweetybert/lib/python3.11/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/george-vengrovski/anaconda3/envs/tweetybert/lib/python3.11/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/george-vengrovski/anaconda3/envs/tweetybert/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/george-vengrovski/anaconda3/envs/tweetybert/lib/python3.11/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch \n",
    "import json\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"src\")\n",
    "os.chdir('/home/george-vengrovski/Documents/projects/tweety_bert_paper')\n",
    "\n",
    "from data_class import SongDataSet_Image\n",
    "from model import TweetyBERT\n",
    "from analysis import plot_umap_projection\n",
    "from utils import detailed_count_parameters, load_weights, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from data_class import SongDataSet_Image, CollateFunction\n",
    "\n",
    "train_dir = \"/home/george-vengrovski/Documents/projects/tweety_bert_paper/files/llb3_train_1_precent\"\n",
    "test_dir = \"/home/george-vengrovski/Documents/projects/tweety_bert_paper/files/llb3_test\"\n",
    "\n",
    "train_dataset = SongDataSet_Image(train_dir, num_classes=21)\n",
    "test_dataset = SongDataSet_Image(test_dir, num_classes=21)\n",
    "\n",
    "collate_fn = CollateFunction(segment_length=1000)  # Adjust the segment length if needed\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_probe import LinearProbeModel, LinearProbeTrainer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "classifier_model = LinearProbeModel(num_classes=21, model_type=\"raw\", model=None, freeze_layers=None, layer_num=None, layer_id=None, classifier_dims=196)\n",
    "classifier_model = classifier_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100: FER = 57.49%, Train Loss = 2.1820, Val Loss = 2.3197\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[39m=\u001b[39m LinearProbeTrainer(model\u001b[39m=\u001b[39mclassifier_model, train_loader\u001b[39m=\u001b[39mtrain_loader, test_loader\u001b[39m=\u001b[39mtest_loader, device\u001b[39m=\u001b[39mdevice, lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m, plotting\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, batches_per_eval\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, desired_total_batches\u001b[39m=\u001b[39m\u001b[39m1e4\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/Documents/projects/tweety_bert_paper/src/linear_probe.py:169\u001b[0m, in \u001b[0;36mLinearProbeTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m total_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    168\u001b[0m \u001b[39mif\u001b[39;00m total_batches \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatches_per_eval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 169\u001b[0m     avg_val_loss, avg_frame_error \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidate_model()\n\u001b[1;32m    171\u001b[0m     raw_loss_list\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m    172\u001b[0m     raw_val_loss_list\u001b[39m.\u001b[39mappend(avg_val_loss)\n",
      "File \u001b[0;32m~/Documents/projects/tweety_bert_paper/src/linear_probe.py:123\u001b[0m, in \u001b[0;36mLinearProbeTrainer.validate_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m num_val_batches \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mfor\u001b[39;00m i, (spectrogram, label) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_loader):\n\u001b[1;32m    124\u001b[0m         \u001b[39mif\u001b[39;00m i \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatches_per_eval:\n\u001b[1;32m    125\u001b[0m             \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tweetybert/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/tweetybert/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/tweetybert/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/tweetybert/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Documents/projects/tweety_bert_paper/src/data_class.py:36\u001b[0m, in \u001b[0;36mSongDataSet_Image.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     34\u001b[0m ground_truth_labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(ground_truth_labels, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint64)\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     35\u001b[0m spectogram \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(spectogram)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m ground_truth_labels \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mone_hot(ground_truth_labels, num_classes\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     38\u001b[0m \u001b[39mreturn\u001b[39;00m spectogram, ground_truth_labels\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = LinearProbeTrainer(model=classifier_model, train_loader=train_loader, test_loader=test_loader, device=device, lr=1e-3, plotting=True, batches_per_eval=100, desired_total_batches=1e4, patience=4)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/142 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  37%|███▋      | 52/142 [00:01<00:03, 26.07batch/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m      5\u001b[0m evaluator \u001b[39m=\u001b[39m ModelEvaluator(model\u001b[39m=\u001b[39mclassifier_model, \n\u001b[1;32m      6\u001b[0m                            test_loader\u001b[39m=\u001b[39mtest_loader, \n\u001b[1;32m      7\u001b[0m                            num_classes\u001b[39m=\u001b[39m\u001b[39m21\u001b[39m,  \u001b[39m# Assuming there are 21 possible classes\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m                            filter_unseen_classes\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,  \u001b[39m# Enable filtering based on training set classes\u001b[39;00m\n\u001b[1;32m     11\u001b[0m                            train_dir\u001b[39m=\u001b[39mtrain_dir)  \u001b[39m# Path to the training dataset directory\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# Perform model validation with multiple passes\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m class_frame_error_rates, total_frame_error_rate \u001b[39m=\u001b[39m evaluator\u001b[39m.\u001b[39mvalidate_model_multiple_passes(num_passes\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, max_batches\u001b[39m=\u001b[39m\u001b[39m1250\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[39m# Save the results to a specified directory, for example 'evaluation_results'\u001b[39;00m\n\u001b[1;32m     17\u001b[0m evaluator\u001b[39m.\u001b[39msave_results(class_frame_error_rates, total_frame_error_rate, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/projects/tweety_bert_paper/src/linear_probe.py:252\u001b[0m, in \u001b[0;36mModelEvaluator.validate_model_multiple_passes\u001b[0;34m(self, num_passes, max_batches)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_passes):\n\u001b[1;32m    251\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 252\u001b[0m         \u001b[39mfor\u001b[39;00m i, (waveform, label) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_loader):\n\u001b[1;32m    253\u001b[0m             \u001b[39mif\u001b[39;00m i \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m max_batches:\n\u001b[1;32m    254\u001b[0m                 \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tweetybert/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/tweetybert/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/tweetybert/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/tweetybert/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Documents/projects/tweety_bert_paper/src/data_class.py:19\u001b[0m, in \u001b[0;36mSongDataSet_Image.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     17\u001b[0m file_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_paths[index]\n\u001b[1;32m     18\u001b[0m data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(file_path, allow_pickle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 19\u001b[0m spectogram \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     20\u001b[0m \u001b[39m# spectogram = spectogram[20:216]\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[39m# Remove when free from Yarden's Spec Gen Method \u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m# Z-score normalization\u001b[39;00m\n\u001b[1;32m     24\u001b[0m mean \u001b[39m=\u001b[39m spectogram\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/anaconda3/envs/tweetybert/lib/python3.11/site-packages/numpy/lib/npyio.py:253\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mif\u001b[39;00m magic \u001b[39m==\u001b[39m \u001b[39mformat\u001b[39m\u001b[39m.\u001b[39mMAGIC_PREFIX:\n\u001b[1;32m    252\u001b[0m     \u001b[39mbytes\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzip\u001b[39m.\u001b[39mopen(key)\n\u001b[0;32m--> 253\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39m\u001b[39m.\u001b[39mread_array(\u001b[39mbytes\u001b[39m,\n\u001b[1;32m    254\u001b[0m                              allow_pickle\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mallow_pickle,\n\u001b[1;32m    255\u001b[0m                              pickle_kwargs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpickle_kwargs,\n\u001b[1;32m    256\u001b[0m                              max_header_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_header_size)\n\u001b[1;32m    257\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzip\u001b[39m.\u001b[39mread(key)\n",
      "File \u001b[0;32m~/anaconda3/envs/tweetybert/lib/python3.11/site-packages/numpy/lib/format.py:821\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    818\u001b[0m max_read_count \u001b[39m=\u001b[39m BUFFER_SIZE \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39mmin\u001b[39m(BUFFER_SIZE, dtype\u001b[39m.\u001b[39mitemsize)\n\u001b[1;32m    820\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, count, max_read_count):\n\u001b[0;32m--> 821\u001b[0m     read_count \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(max_read_count, count \u001b[39m-\u001b[39m i)\n\u001b[1;32m    822\u001b[0m     read_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(read_count \u001b[39m*\u001b[39m dtype\u001b[39m.\u001b[39mitemsize)\n\u001b[1;32m    823\u001b[0m     data \u001b[39m=\u001b[39m _read_bytes(fp, read_size, \u001b[39m\"\u001b[39m\u001b[39marray data\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from linear_probe import ModelEvaluator\n",
    "\n",
    "\n",
    "# Initialize the ModelEvaluator with the filter_unseen_classes feature\n",
    "evaluator = ModelEvaluator(model=classifier_model, \n",
    "                           test_loader=test_loader, \n",
    "                           num_classes=21,  # Assuming there are 21 possible classes\n",
    "                           device='cuda:0',  # Use CUDA if available\n",
    "                           use_tqdm=True,  # Enable a progress bar for the evaluation\n",
    "                           filter_unseen_classes=True,  # Enable filtering based on training set classes\n",
    "                           train_dir=train_dir)  # Path to the training dataset directory\n",
    "\n",
    "# Perform model validation with multiple passes\n",
    "class_frame_error_rates, total_frame_error_rate = evaluator.validate_model_multiple_passes(num_passes=1, max_batches=1250)\n",
    "\n",
    "# Save the results to a specified directory, for example 'evaluation_results'\n",
    "evaluator.save_results(class_frame_error_rates, total_frame_error_rate, '')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canary-vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
